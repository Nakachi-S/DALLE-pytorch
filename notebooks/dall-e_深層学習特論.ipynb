{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dalle-pytorch\n",
      "  Downloading dalle_pytorch-0.12.5-py3-none-any.whl (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 4.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting transformers\n",
      "  Downloading transformers-4.6.1-py3-none-any.whl (2.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2 MB 9.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /home/nakachi/miniconda3/envs/env/lib/python3.6/site-packages (from dalle-pytorch) (4.50.2)\n",
      "Requirement already satisfied: torchvision in /home/nakachi/miniconda3/envs/env/lib/python3.6/site-packages (from dalle-pytorch) (0.7.0)\n",
      "Requirement already satisfied: torch>=1.6 in /home/nakachi/miniconda3/envs/env/lib/python3.6/site-packages (from dalle-pytorch) (1.6.0)\n",
      "Collecting taming-transformers\n",
      "  Downloading taming_transformers-0.0.1-py3-none-any.whl (45 kB)\n",
      "\u001b[K     |████████████████████████████████| 45 kB 6.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting g-mlp-pytorch\n",
      "  Downloading g_mlp_pytorch-0.0.16-py3-none-any.whl (5.2 kB)\n",
      "Requirement already satisfied: pillow in /home/nakachi/miniconda3/envs/env/lib/python3.6/site-packages (from dalle-pytorch) (8.0.0)\n",
      "Collecting ftfy\n",
      "  Downloading ftfy-6.0.3.tar.gz (64 kB)\n",
      "\u001b[K     |████████████████████████████████| 64 kB 6.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting youtokentome\n",
      "  Downloading youtokentome-1.0.6-cp36-cp36m-manylinux2010_x86_64.whl (1.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.7 MB 33.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting einops>=0.3\n",
      "  Downloading einops-0.3.0-py2.py3-none-any.whl (25 kB)\n",
      "Collecting axial-positional-embedding\n",
      "  Downloading axial_positional_embedding-0.2.1.tar.gz (2.6 kB)\n",
      "Collecting tokenizers\n",
      "  Downloading tokenizers-0.10.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 37.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting DALL-E\n",
      "  Downloading DALL_E-0.1-py3-none-any.whl (6.0 kB)\n",
      "Requirement already satisfied: regex in /home/nakachi/miniconda3/envs/env/lib/python3.6/site-packages (from dalle-pytorch) (2020.10.28)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: requests in /home/nakachi/miniconda3/envs/env/lib/python3.6/site-packages (from transformers->dalle-pytorch) (2.24.0)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 32.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting huggingface-hub==0.0.8\n",
      "  Downloading huggingface_hub-0.0.8-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: packaging in /home/nakachi/miniconda3/envs/env/lib/python3.6/site-packages (from transformers->dalle-pytorch) (20.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/nakachi/miniconda3/envs/env/lib/python3.6/site-packages (from transformers->dalle-pytorch) (1.19.5)\n",
      "Collecting dataclasses; python_version < \"3.7\"\n",
      "  Downloading dataclasses-0.8-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/nakachi/miniconda3/envs/env/lib/python3.6/site-packages (from transformers->dalle-pytorch) (2.0.0)\n",
      "Collecting future\n",
      "  Downloading future-0.18.2.tar.gz (829 kB)\n",
      "\u001b[K     |████████████████████████████████| 829 kB 33.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting omegaconf>=2.0.0\n",
      "  Downloading omegaconf-2.1.0-py3-none-any.whl (74 kB)\n",
      "\u001b[K     |████████████████████████████████| 74 kB 6.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting pytorch-lightning>=1.0.8\n",
      "  Downloading pytorch_lightning-1.3.4-py3-none-any.whl (806 kB)\n",
      "\u001b[K     |████████████████████████████████| 806 kB 35.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wcwidth in /home/nakachi/miniconda3/envs/env/lib/python3.6/site-packages (from ftfy->dalle-pytorch) (0.2.5)\n",
      "Requirement already satisfied: Click>=7.0 in /home/nakachi/miniconda3/envs/env/lib/python3.6/site-packages (from youtokentome->dalle-pytorch) (7.1.2)\n",
      "Collecting pytest\n",
      "  Downloading pytest-6.2.4-py3-none-any.whl (280 kB)\n",
      "\u001b[K     |████████████████████████████████| 280 kB 41.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting blobfile\n",
      "  Downloading blobfile-0.11.0-py3-none-any.whl (32 kB)\n",
      "Collecting mypy\n",
      "  Downloading mypy-0.901-cp36-cp36m-manylinux2010_x86_64.whl (21.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 21.1 MB 30.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /home/nakachi/miniconda3/envs/env/lib/python3.6/site-packages (from requests->transformers->dalle-pytorch) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/nakachi/miniconda3/envs/env/lib/python3.6/site-packages (from requests->transformers->dalle-pytorch) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/nakachi/miniconda3/envs/env/lib/python3.6/site-packages (from requests->transformers->dalle-pytorch) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/nakachi/miniconda3/envs/env/lib/python3.6/site-packages (from requests->transformers->dalle-pytorch) (2020.6.20)\n",
      "Requirement already satisfied: six in /home/nakachi/miniconda3/envs/env/lib/python3.6/site-packages (from sacremoses->transformers->dalle-pytorch) (1.15.0)\n",
      "Requirement already satisfied: joblib in /home/nakachi/miniconda3/envs/env/lib/python3.6/site-packages (from sacremoses->transformers->dalle-pytorch) (0.17.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/nakachi/miniconda3/envs/env/lib/python3.6/site-packages (from packaging->transformers->dalle-pytorch) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/nakachi/miniconda3/envs/env/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->transformers->dalle-pytorch) (3.3.1)\n",
      "Requirement already satisfied: PyYAML>=5.1.* in /home/nakachi/miniconda3/envs/env/lib/python3.6/site-packages (from omegaconf>=2.0.0->taming-transformers->dalle-pytorch) (5.3.1)\n",
      "Collecting antlr4-python3-runtime==4.8\n",
      "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
      "\u001b[K     |████████████████████████████████| 112 kB 50.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard!=2.5.0,>=2.2.0\n",
      "  Downloading tensorboard-2.4.1-py3-none-any.whl (10.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.6 MB 42.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyDeprecate==0.3.0\n",
      "  Downloading pyDeprecate-0.3.0-py3-none-any.whl (10 kB)\n",
      "Collecting fsspec[http]>=2021.4.0\n",
      "  Downloading fsspec-2021.6.0-py3-none-any.whl (114 kB)\n",
      "\u001b[K     |████████████████████████████████| 114 kB 36.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torchmetrics>=0.2.0\n",
      "  Downloading torchmetrics-0.3.2-py3-none-any.whl (274 kB)\n",
      "\u001b[K     |████████████████████████████████| 274 kB 64.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting py>=1.8.2\n",
      "  Downloading py-1.10.0-py2.py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 14.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pluggy<1.0.0a1,>=0.12\n",
      "  Downloading pluggy-0.13.1-py2.py3-none-any.whl (18 kB)\n",
      "Collecting iniconfig\n",
      "  Downloading iniconfig-1.1.1-py2.py3-none-any.whl (5.0 kB)\n",
      "Collecting toml\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /home/nakachi/miniconda3/envs/env/lib/python3.6/site-packages (from pytest->DALL-E->dalle-pytorch) (20.2.0)\n",
      "Collecting xmltodict~=0.12.0\n",
      "  Downloading xmltodict-0.12.0-py2.py3-none-any.whl (9.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.1 in /home/nakachi/miniconda3/envs/env/lib/python3.6/site-packages (from blobfile->DALL-E->dalle-pytorch) (3.7.4.3)\n",
      "Collecting pycryptodomex~=3.8\n",
      "  Downloading pycryptodomex-3.10.1-cp35-abi3-manylinux2010_x86_64.whl (1.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.9 MB 63.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting typed-ast<1.5.0,>=1.4.0; python_version < \"3.8\"\n",
      "  Downloading typed_ast-1.4.3-cp36-cp36m-manylinux1_x86_64.whl (743 kB)\n",
      "\u001b[K     |████████████████████████████████| 743 kB 58.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting mypy-extensions<0.5.0,>=0.4.3\n",
      "  Downloading mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/nakachi/miniconda3/envs/env/lib/python3.6/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.0.8->taming-transformers->dalle-pytorch) (50.3.0.post20201006)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/nakachi/miniconda3/envs/env/lib/python3.6/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.0.8->taming-transformers->dalle-pytorch) (1.7.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/nakachi/miniconda3/envs/env/lib/python3.6/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.0.8->taming-transformers->dalle-pytorch) (1.24.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/nakachi/miniconda3/envs/env/lib/python3.6/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.0.8->taming-transformers->dalle-pytorch) (1.0.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/nakachi/miniconda3/envs/env/lib/python3.6/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.0.8->taming-transformers->dalle-pytorch) (0.4.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/nakachi/miniconda3/envs/env/lib/python3.6/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.0.8->taming-transformers->dalle-pytorch) (0.11.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/nakachi/miniconda3/envs/env/lib/python3.6/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.0.8->taming-transformers->dalle-pytorch) (3.3.3)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /home/nakachi/miniconda3/envs/env/lib/python3.6/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.0.8->taming-transformers->dalle-pytorch) (1.32.0)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/nakachi/miniconda3/envs/env/lib/python3.6/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.0.8->taming-transformers->dalle-pytorch) (0.35.1)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /home/nakachi/miniconda3/envs/env/lib/python3.6/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.0.8->taming-transformers->dalle-pytorch) (3.13.0)\n",
      "Collecting aiohttp; extra == \"http\"\n",
      "  Downloading aiohttp-3.7.4.post0-cp36-cp36m-manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 38.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyasn1-modules>=0.2.1 in /home/nakachi/miniconda3/envs/env/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.0.8->taming-transformers->dalle-pytorch) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /home/nakachi/miniconda3/envs/env/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.0.8->taming-transformers->dalle-pytorch) (4.6)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/nakachi/miniconda3/envs/env/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.0.8->taming-transformers->dalle-pytorch) (4.2.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/nakachi/miniconda3/envs/env/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.0.8->taming-transformers->dalle-pytorch) (1.3.0)\n",
      "Collecting async-timeout<4.0,>=3.0\n",
      "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-5.1.0-cp36-cp36m-manylinux2014_x86_64.whl (141 kB)\n",
      "\u001b[K     |████████████████████████████████| 141 kB 36.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting idna-ssl>=1.0; python_version < \"3.7\"\n",
      "  Downloading idna-ssl-1.1.0.tar.gz (3.4 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.6.3-cp36-cp36m-manylinux2014_x86_64.whl (293 kB)\n",
      "\u001b[K     |████████████████████████████████| 293 kB 38.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/nakachi/miniconda3/envs/env/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.0.8->taming-transformers->dalle-pytorch) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/nakachi/miniconda3/envs/env/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.0.8->taming-transformers->dalle-pytorch) (3.1.0)\n",
      "Building wheels for collected packages: ftfy, axial-positional-embedding, future, antlr4-python3-runtime, idna-ssl\n",
      "  Building wheel for ftfy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ftfy: filename=ftfy-6.0.3-py3-none-any.whl size=41914 sha256=2a7a4139f0d1fbaaf282ac359ceebac6136e41780584f016db268014c305e695\n",
      "  Stored in directory: /home/nakachi/.cache/pip/wheels/ff/2a/24/75041425faf3347ab146a4a3d0484f723b2c44a7966a06e3f0\n",
      "  Building wheel for axial-positional-embedding (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for axial-positional-embedding: filename=axial_positional_embedding-0.2.1-py3-none-any.whl size=2903 sha256=402daacd02ceb773162c79569c8db1c5592ebeae35698948978f9c8fc5c2bef5\n",
      "  Stored in directory: /home/nakachi/.cache/pip/wheels/87/1e/cd/b5de135ee3faf0c4c525553227e601b0e7739264014dc3e98f\n",
      "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491059 sha256=a3359bf1ec50c8423ed3cdf8b7b29dd03701884b6c182a266e6a344a212fbbdb\n",
      "  Stored in directory: /home/nakachi/.cache/pip/wheels/6e/9c/ed/4499c9865ac1002697793e0ae05ba6be33553d098f3347fb94\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141229 sha256=b4f2c675704afd8f79a0624607c73e576de91daf4e651514d377383fed39b8fa\n",
      "  Stored in directory: /home/nakachi/.cache/pip/wheels/a8/04/35/9449686f1c26ff16f6224dc942e108329f3782185802ec6b93\n",
      "  Building wheel for idna-ssl (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for idna-ssl: filename=idna_ssl-1.1.0-py3-none-any.whl size=3161 sha256=f3090b8b9e636ef13acc6b7ff3ee2cbbe00a339580c007a5fb97d0a0bdcb8bad\n",
      "  Stored in directory: /home/nakachi/.cache/pip/wheels/6a/f5/9c/f8331a854f7a8739cf0e74c13854e4dd7b1af11b04fe1dde13\n",
      "Successfully built ftfy axial-positional-embedding future antlr4-python3-runtime idna-ssl\n",
      "Installing collected packages: filelock, sacremoses, huggingface-hub, tokenizers, dataclasses, transformers, antlr4-python3-runtime, omegaconf, tensorboard, pyDeprecate, async-timeout, multidict, idna-ssl, yarl, aiohttp, fsspec, future, torchmetrics, pytorch-lightning, taming-transformers, einops, g-mlp-pytorch, ftfy, youtokentome, axial-positional-embedding, py, pluggy, iniconfig, toml, pytest, xmltodict, pycryptodomex, blobfile, typed-ast, mypy-extensions, mypy, DALL-E, dalle-pytorch\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 1.14.0\n",
      "    Uninstalling tensorboard-1.14.0:\n",
      "      Successfully uninstalled tensorboard-1.14.0\n",
      "\u001b[31mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "tensorflow 2.0.0 requires tensorboard<2.1.0,>=2.0.0, but you'll have tensorboard 2.4.1 which is incompatible.\n",
      "tensorflow 2.0.0 requires tensorflow-estimator<2.1.0,>=2.0.0, but you'll have tensorflow-estimator 1.14.0 which is incompatible.\n",
      "tensorflow-gpu 1.14.0 requires tensorboard<1.15.0,>=1.14.0, but you'll have tensorboard 2.4.1 which is incompatible.\u001b[0m\n",
      "Successfully installed DALL-E-0.1 aiohttp-3.7.4.post0 antlr4-python3-runtime-4.8 async-timeout-3.0.1 axial-positional-embedding-0.2.1 blobfile-0.11.0 dalle-pytorch-0.12.5 dataclasses-0.8 einops-0.3.0 filelock-3.0.12 fsspec-2021.6.0 ftfy-6.0.3 future-0.18.2 g-mlp-pytorch-0.0.16 huggingface-hub-0.0.8 idna-ssl-1.1.0 iniconfig-1.1.1 multidict-5.1.0 mypy-0.901 mypy-extensions-0.4.3 omegaconf-2.1.0 pluggy-0.13.1 py-1.10.0 pyDeprecate-0.3.0 pycryptodomex-3.10.1 pytest-6.2.4 pytorch-lightning-1.3.4 sacremoses-0.0.45 taming-transformers-0.0.1 tensorboard-2.4.1 tokenizers-0.10.3 toml-0.10.2 torchmetrics-0.3.2 transformers-4.6.1 typed-ast-1.4.3 xmltodict-0.12.0 yarl-1.6.3 youtokentome-1.0.6\n"
     ]
    }
   ],
   "source": [
    "!pip install dalle-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wandb\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:  ········································\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/nakachi/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gdown\n",
      "  Downloading gdown-3.13.0.tar.gz (9.3 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six in /home/nakachi/miniconda3/envs/env/lib/python3.6/site-packages (from gdown) (1.15.0)\n",
      "Requirement already satisfied: tqdm in /home/nakachi/miniconda3/envs/env/lib/python3.6/site-packages (from gdown) (4.50.2)\n",
      "Requirement already satisfied: filelock in /home/nakachi/miniconda3/envs/env/lib/python3.6/site-packages (from gdown) (3.0.12)\n",
      "Requirement already satisfied: requests[socks]>=2.12.0 in /home/nakachi/miniconda3/envs/env/lib/python3.6/site-packages (from gdown) (2.24.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/nakachi/miniconda3/envs/env/lib/python3.6/site-packages (from requests[socks]>=2.12.0->gdown) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/nakachi/miniconda3/envs/env/lib/python3.6/site-packages (from requests[socks]>=2.12.0->gdown) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/nakachi/miniconda3/envs/env/lib/python3.6/site-packages (from requests[socks]>=2.12.0->gdown) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/nakachi/miniconda3/envs/env/lib/python3.6/site-packages (from requests[socks]>=2.12.0->gdown) (2020.6.20)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /home/nakachi/miniconda3/envs/env/lib/python3.6/site-packages (from requests[socks]>=2.12.0->gdown) (1.7.1)\n",
      "Building wheels for collected packages: gdown\n",
      "  Building wheel for gdown (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gdown: filename=gdown-3.13.0-py3-none-any.whl size=9034 sha256=1e6cde5ddb37f8ee1dcad6510e59a199f890ead72da76352f80d4e99b8643cc6\n",
      "  Stored in directory: /home/nakachi/.cache/pip/wheels/6a/87/bd/09b16161b149fd6711ac76b5420d78ed58bd6a320e892117c3\n",
      "Successfully built gdown\n",
      "Installing collected packages: gdown\n",
      "Successfully installed gdown-3.13.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1vF8Ht0VThpobtmShD52_INhpIgy6eEXq\n",
      "To: /home/nakachi/DALLE-pytorch/notebooks/CUB_200_2011.tgz\n",
      "1.15GB [00:34, 33.1MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1kaIqFwTLD7Ml3ib9NQpjoUSD4FUD21-I\n",
      "To: /home/nakachi/DALLE-pytorch/notebooks/birds.zip\n",
      "613MB [00:20, 30.2MB/s] \n"
     ]
    }
   ],
   "source": [
    "!gdown https://drive.google.com/uc?id=1vF8Ht0VThpobtmShD52_INhpIgy6eEXq\n",
    "!gdown https://drive.google.com/uc?id=1kaIqFwTLD7Ml3ib9NQpjoUSD4FUD21-I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf birds CUB_200_2011\n",
    "!unzip birds.zip\n",
    "!tar zxvf CUB_200_2011.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from math import sqrt\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "# vision imports\n",
    "\n",
    "from torchvision import transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "# dalle classes\n",
    "\n",
    "from dalle_pytorch import DiscreteVAE\n",
    "\n",
    "# constants\n",
    "\n",
    "IMAGE_SIZE = 128\n",
    "IMAGE_PATH = './'\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 1e-3\n",
    "LR_DECAY_RATE = 0.98\n",
    "\n",
    "NUM_TOKENS = 8192\n",
    "NUM_LAYERS = 2\n",
    "NUM_RESNET_BLOCKS = 2\n",
    "SMOOTH_L1_LOSS = False\n",
    "EMB_DIM = 512\n",
    "HID_DIM = 256\n",
    "KL_LOSS_WEIGHT = 0\n",
    "\n",
    "STARTING_TEMP = 1.\n",
    "TEMP_MIN = 0.5\n",
    "ANNEAL_RATE = 1e-6\n",
    "\n",
    "NUM_IMAGES_SAVE = 4\n",
    "\n",
    "# data\n",
    "\n",
    "ds = ImageFolder(\n",
    "    IMAGE_PATH,\n",
    "    T.Compose([\n",
    "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "        T.Resize(IMAGE_SIZE),\n",
    "        T.CenterCrop(IMAGE_SIZE),\n",
    "        T.ToTensor()\n",
    "    ])\n",
    ")\n",
    "\n",
    "dl = DataLoader(ds, BATCH_SIZE, shuffle = True)\n",
    "\n",
    "vae_params = dict(\n",
    "    image_size = IMAGE_SIZE,\n",
    "    num_layers = NUM_LAYERS,\n",
    "    num_tokens = NUM_TOKENS,\n",
    "    codebook_dim = EMB_DIM,\n",
    "    hidden_dim   = HID_DIM,\n",
    "    num_resnet_blocks = NUM_RESNET_BLOCKS\n",
    ")\n",
    "\n",
    "vae = DiscreteVAE(\n",
    "    **vae_params,\n",
    "    smooth_l1_loss = SMOOTH_L1_LOSS,\n",
    "    kl_div_loss_weight = KL_LOSS_WEIGHT\n",
    ").cuda()\n",
    "\n",
    "\n",
    "assert len(ds) > 0, 'folder does not contain any images'\n",
    "print(f'{len(ds)} images found for training')\n",
    "\n",
    "def save_model(path):\n",
    "    save_obj = {\n",
    "        'hparams': vae_params,\n",
    "        'weights': vae.state_dict()\n",
    "    }\n",
    "\n",
    "    torch.save(save_obj, path)\n",
    "\n",
    "# optimizer\n",
    "\n",
    "opt = Adam(vae.parameters(), lr = LEARNING_RATE)\n",
    "sched = ExponentialLR(optimizer = opt, gamma = LR_DECAY_RATE)\n",
    "\n",
    "# weights & biases experiment tracking\n",
    "\n",
    "import wandb\n",
    "\n",
    "model_config = dict(\n",
    "    num_tokens = NUM_TOKENS,\n",
    "    smooth_l1_loss = SMOOTH_L1_LOSS,\n",
    "    num_resnet_blocks = NUM_RESNET_BLOCKS,\n",
    "    kl_loss_weight = KL_LOSS_WEIGHT\n",
    ")\n",
    "\n",
    "run = wandb.init(\n",
    "    project = 'dalle_train_vae',\n",
    "    job_type = 'train_model',\n",
    "    config = model_config\n",
    ")\n",
    "\n",
    "# starting temperature\n",
    "\n",
    "global_step = 0\n",
    "temp = STARTING_TEMP\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for i, (images, _) in enumerate(dl):\n",
    "        images = images.cuda()\n",
    "\n",
    "        loss, recons = vae(\n",
    "            images,\n",
    "            return_loss = True,\n",
    "            return_recons = True,\n",
    "            temp = temp\n",
    "        )\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        logs = {}\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            k = NUM_IMAGES_SAVE\n",
    "\n",
    "            with torch.no_grad():\n",
    "                codes = vae.get_codebook_indices(images[:k])\n",
    "                hard_recons = vae.decode(codes)\n",
    "\n",
    "            images, recons = map(lambda t: t[:k], (images, recons))\n",
    "            images, recons, hard_recons, codes = map(lambda t: t.detach().cpu(), (images, recons, hard_recons, codes))\n",
    "            images, recons, hard_recons = map(lambda t: make_grid(t.float(), nrow = int(sqrt(k)), normalize = True, range = (-1, 1)), (images, recons, hard_recons))\n",
    "\n",
    "            logs = {\n",
    "                **logs,\n",
    "                'sample images':        wandb.Image(images, caption = 'original images'),\n",
    "                'reconstructions':      wandb.Image(recons, caption = 'reconstructions'),\n",
    "                'hard reconstructions': wandb.Image(hard_recons, caption = 'hard reconstructions'),\n",
    "                'codebook_indices':     wandb.Histogram(codes),\n",
    "                'temperature':          temp\n",
    "            }\n",
    "\n",
    "            save_model(f'./vae.pt')\n",
    "            wandb.save('./vae.pt')\n",
    "\n",
    "            # temperature anneal\n",
    "\n",
    "            temp = max(temp * math.exp(-ANNEAL_RATE * global_step), TEMP_MIN)\n",
    "\n",
    "            # lr decay\n",
    "\n",
    "            sched.step()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            lr = sched.get_last_lr()[0]\n",
    "            print(epoch, i, f'lr - {lr:6f} loss - {loss.item()}')\n",
    "\n",
    "            logs = {\n",
    "                **logs,\n",
    "                'epoch': epoch,\n",
    "                'iter': i,\n",
    "                'loss': loss.item(),\n",
    "                'lr': lr\n",
    "            }\n",
    "\n",
    "        wandb.log(logs)\n",
    "        global_step += 1\n",
    "\n",
    "    # save trained model to wandb as an artifact every epoch's end\n",
    "\n",
    "    model_artifact = wandb.Artifact('trained-vae', type = 'model', metadata = dict(model_config))\n",
    "    model_artifact.add_file('vae.pt')\n",
    "    run.log_artifact(model_artifact)\n",
    "\n",
    "# save final vae and cleanup\n",
    "\n",
    "save_model('./vae-final.pt')\n",
    "wandb.save('./vae-final.pt')\n",
    "\n",
    "model_artifact = wandb.Artifact('trained-vae', type = 'model', metadata = dict(model_config))\n",
    "model_artifact.add_file('vae-final.pt')\n",
    "run.log_artifact(model_artifact)\n",
    "\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using pretrained VAE for encoding images to tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████| 215185363/215185363 [01:00<00:00, 3576736.81it/s]\n",
      "100%|████████████████████████| 175360231/175360231 [00:32<00:00, 5408733.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11788 image-text pairs found for training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnakachi-s\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.31<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">astral-haze-3</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/nakachi-s/dalle_train_transformer\" target=\"_blank\">https://wandb.ai/nakachi-s/dalle_train_transformer</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/nakachi-s/dalle_train_transformer/runs/f8hnnw6e\" target=\"_blank\">https://wandb.ai/nakachi-s/dalle_train_transformer/runs/f8hnnw6e</a><br/>\n",
       "                Run data is saved locally in <code>/home/nakachi/DALLE-pytorch/notebooks/wandb/run-20210609_093859-f8hnnw6e</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 loss - 9.411307334899902\n",
      "0 10 loss - 9.197771072387695\n",
      "0 20 loss - 9.0065336227417\n",
      "0 30 loss - 8.774468421936035\n",
      "0 40 loss - 8.47704792022705\n",
      "0 50 loss - 8.048527717590332\n",
      "0 60 loss - 7.711822032928467\n",
      "0 70 loss - 7.688018321990967\n",
      "0 80 loss - 7.687277317047119\n",
      "0 90 loss - 7.8614726066589355\n",
      "0 100 loss - 7.7845234870910645\n",
      "0 110 loss - 7.805528163909912\n",
      "0 120 loss - 7.855661869049072\n",
      "0 130 loss - 7.56503963470459\n",
      "0 140 loss - 7.761192321777344\n",
      "0 150 loss - 7.647830486297607\n",
      "0 160 loss - 7.597595691680908\n",
      "0 170 loss - 7.574670314788818\n",
      "0 180 loss - 7.8922014236450195\n",
      "0 190 loss - 7.843993663787842\n",
      "0 200 loss - 7.737320899963379\n",
      "0 210 loss - 7.724544525146484\n",
      "0 220 loss - 7.8991265296936035\n",
      "0 230 loss - 7.631166934967041\n",
      "0 240 loss - 7.627883434295654\n",
      "0 250 loss - 7.477207660675049\n",
      "0 260 loss - 7.693474292755127\n",
      "0 270 loss - 7.832309246063232\n",
      "0 280 loss - 7.734604358673096\n",
      "0 290 loss - 7.75332498550415\n",
      "0 300 loss - 7.706338882446289\n",
      "0 310 loss - 7.584452152252197\n",
      "0 320 loss - 7.6088643074035645\n",
      "0 330 loss - 7.506640434265137\n",
      "0 340 loss - 7.501192569732666\n",
      "0 350 loss - 7.611975193023682\n",
      "0 360 loss - 7.474465847015381\n",
      "0 370 loss - 7.7603278160095215\n",
      "0 380 loss - 7.705155849456787\n",
      "0 390 loss - 8.134142875671387\n",
      "0 400 loss - 7.585555076599121\n",
      "0 410 loss - 7.70310640335083\n",
      "0 420 loss - 7.666562080383301\n",
      "0 430 loss - 7.579984188079834\n",
      "0 440 loss - 7.420384407043457\n",
      "0 450 loss - 7.467641353607178\n",
      "0 460 loss - 7.401852130889893\n",
      "0 470 loss - 7.748735427856445\n",
      "0 480 loss - 7.580632209777832\n",
      "0 490 loss - 7.579479217529297\n",
      "0 500 loss - 7.782299995422363\n",
      "0 510 loss - 7.699625492095947\n",
      "0 520 loss - 7.7316718101501465\n",
      "0 530 loss - 7.668496608734131\n",
      "0 540 loss - 7.698249816894531\n",
      "0 550 loss - 7.734918594360352\n",
      "0 560 loss - 7.667555809020996\n",
      "0 570 loss - 7.591136455535889\n",
      "0 580 loss - 7.703622817993164\n",
      "0 590 loss - 7.9058990478515625\n",
      "0 600 loss - 7.288737773895264\n",
      "0 610 loss - 7.665294170379639\n",
      "0 620 loss - 7.776238441467285\n",
      "0 630 loss - 7.720000267028809\n",
      "0 640 loss - 7.6851043701171875\n",
      "0 650 loss - 7.571486949920654\n",
      "0 660 loss - 7.712149620056152\n",
      "0 670 loss - 7.46387243270874\n",
      "0 680 loss - 7.506579875946045\n",
      "0 690 loss - 7.694145202636719\n",
      "0 700 loss - 7.601189136505127\n",
      "0 710 loss - 7.509073734283447\n",
      "0 720 loss - 7.665035724639893\n",
      "0 730 loss - 7.432647228240967\n",
      "0 740 loss - 7.7765116691589355\n",
      "0 750 loss - 7.633362770080566\n",
      "0 760 loss - 7.638514041900635\n",
      "0 770 loss - 7.562782287597656\n",
      "0 780 loss - 7.656795024871826\n",
      "0 790 loss - 7.627719402313232\n",
      "0 800 loss - 7.720657825469971\n",
      "0 810 loss - 7.351566314697266\n",
      "0 820 loss - 7.663565158843994\n",
      "0 830 loss - 7.383635997772217\n",
      "0 840 loss - 7.533195495605469\n",
      "0 850 loss - 7.403402328491211\n",
      "0 860 loss - 7.956692218780518\n",
      "0 870 loss - 7.263792037963867\n",
      "0 880 loss - 7.715358257293701\n",
      "0 890 loss - 7.738224506378174\n",
      "0 900 loss - 7.647874355316162\n",
      "0 910 loss - 7.491142272949219\n",
      "0 920 loss - 7.7716898918151855\n",
      "0 930 loss - 7.712310314178467\n",
      "0 940 loss - 7.489569664001465\n",
      "0 950 loss - 7.590556621551514\n",
      "0 960 loss - 7.714980125427246\n",
      "0 970 loss - 7.368183135986328\n",
      "0 980 loss - 7.685345649719238\n",
      "0 990 loss - 7.5329790115356445\n",
      "0 1000 loss - 7.738874912261963\n",
      "0 1010 loss - 7.235751628875732\n",
      "0 1020 loss - 7.623423099517822\n",
      "0 1030 loss - 7.3298234939575195\n",
      "0 1040 loss - 7.689706325531006\n",
      "0 1050 loss - 7.520689964294434\n",
      "0 1060 loss - 7.615706920623779\n",
      "0 1070 loss - 7.446167945861816\n",
      "0 1080 loss - 7.201017379760742\n",
      "0 1090 loss - 7.623005390167236\n",
      "0 1100 loss - 7.667447566986084\n",
      "0 1110 loss - 7.692525863647461\n",
      "0 1120 loss - 7.681022644042969\n",
      "0 1130 loss - 7.754733562469482\n",
      "0 1140 loss - 7.633711814880371\n",
      "0 1150 loss - 7.481626510620117\n",
      "0 1160 loss - 7.550214767456055\n",
      "0 1170 loss - 7.695793628692627\n",
      "0 1180 loss - 7.663635730743408\n",
      "0 1190 loss - 6.970070838928223\n",
      "0 1200 loss - 7.633129119873047\n",
      "0 1210 loss - 7.793710231781006\n",
      "0 1220 loss - 7.395320892333984\n",
      "0 1230 loss - 7.53439474105835\n",
      "0 1240 loss - 7.552306652069092\n",
      "0 1250 loss - 7.39107608795166\n",
      "0 1260 loss - 7.739321708679199\n",
      "0 1270 loss - 7.480867862701416\n",
      "0 1280 loss - 7.582582950592041\n",
      "0 1290 loss - 7.691997528076172\n",
      "0 1300 loss - 7.525725364685059\n",
      "0 1310 loss - 7.380147457122803\n",
      "0 1320 loss - 7.428404808044434\n",
      "0 1330 loss - 7.621775150299072\n",
      "0 1340 loss - 7.371825695037842\n",
      "0 1350 loss - 7.383267402648926\n",
      "0 1360 loss - 7.577323913574219\n",
      "0 1370 loss - 7.592418193817139\n",
      "0 1380 loss - 7.480294704437256\n",
      "0 1390 loss - 7.4519476890563965\n",
      "0 1400 loss - 7.370493412017822\n",
      "0 1410 loss - 7.452935695648193\n",
      "0 1420 loss - 7.196324348449707\n",
      "0 1430 loss - 7.476181983947754\n",
      "0 1440 loss - 7.496894836425781\n",
      "0 1450 loss - 7.376719951629639\n",
      "0 1460 loss - 7.537064075469971\n",
      "0 1470 loss - 7.733926773071289\n",
      "0 1480 loss - 7.470737457275391\n",
      "0 1490 loss - 7.588096618652344\n",
      "0 1500 loss - 7.281746864318848\n",
      "0 1510 loss - 7.405081272125244\n",
      "0 1520 loss - 7.507849216461182\n",
      "0 1530 loss - 7.687305927276611\n",
      "0 1540 loss - 7.360119819641113\n",
      "0 1550 loss - 7.584615230560303\n",
      "0 1560 loss - 7.086300373077393\n",
      "0 1570 loss - 7.552960395812988\n",
      "0 1580 loss - 7.478095054626465\n",
      "0 1590 loss - 7.581113338470459\n",
      "0 1600 loss - 6.711793422698975\n",
      "0 1610 loss - 7.290478706359863\n",
      "0 1620 loss - 7.523072242736816\n",
      "0 1630 loss - 7.428008556365967\n",
      "0 1640 loss - 7.768800735473633\n",
      "0 1650 loss - 7.475589752197266\n",
      "0 1660 loss - 7.531699180603027\n",
      "0 1670 loss - 7.480237007141113\n",
      "0 1680 loss - 7.69140100479126\n",
      "0 1690 loss - 7.456503868103027\n",
      "0 1700 loss - 7.523651599884033\n",
      "0 1710 loss - 7.592247486114502\n",
      "0 1720 loss - 7.367508411407471\n",
      "0 1730 loss - 7.362848281860352\n",
      "0 1740 loss - 7.75167179107666\n",
      "0 1750 loss - 7.699504852294922\n",
      "0 1760 loss - 7.68979549407959\n",
      "0 1770 loss - 7.520353317260742\n",
      "0 1780 loss - 7.475504398345947\n",
      "0 1790 loss - 7.630563735961914\n",
      "0 1800 loss - 7.467175006866455\n",
      "0 1810 loss - 7.410862922668457\n",
      "0 1820 loss - 7.320401191711426\n",
      "0 1830 loss - 7.430931091308594\n",
      "0 1840 loss - 7.476605415344238\n",
      "0 1850 loss - 7.371682643890381\n",
      "0 1860 loss - 7.713959217071533\n",
      "0 1870 loss - 7.701961994171143\n",
      "0 1880 loss - 7.545559406280518\n",
      "0 1890 loss - 7.2885422706604\n",
      "0 1900 loss - 7.555042743682861\n",
      "0 1910 loss - 7.425381183624268\n",
      "0 1920 loss - 7.46142053604126\n",
      "0 1930 loss - 7.3203606605529785\n",
      "0 1940 loss - 7.520317077636719\n",
      "0 1950 loss - 7.787892818450928\n",
      "0 1960 loss - 7.434167385101318\n",
      "0 1970 loss - 7.390761852264404\n",
      "0 1980 loss - 7.323862075805664\n",
      "0 1990 loss - 7.062986373901367\n",
      "0 2000 loss - 7.425708770751953\n",
      "0 2010 loss - 7.419320583343506\n",
      "0 2020 loss - 7.665410995483398\n",
      "0 2030 loss - 7.537407398223877\n",
      "0 2040 loss - 7.442439556121826\n",
      "0 2050 loss - 7.674101829528809\n",
      "0 2060 loss - 7.436460018157959\n",
      "0 2070 loss - 7.111148357391357\n",
      "0 2080 loss - 7.17484188079834\n",
      "0 2090 loss - 7.367932319641113\n",
      "0 2100 loss - 7.48699426651001\n",
      "0 2110 loss - 7.433748245239258\n",
      "0 2120 loss - 7.359688758850098\n",
      "0 2130 loss - 7.3806962966918945\n",
      "0 2140 loss - 7.669376373291016\n",
      "0 2150 loss - 7.048921585083008\n",
      "0 2160 loss - 7.593688011169434\n",
      "0 2170 loss - 7.360637664794922\n",
      "0 2180 loss - 7.396270275115967\n",
      "0 2190 loss - 7.291970729827881\n",
      "0 2200 loss - 7.381267070770264\n",
      "0 2210 loss - 7.115736961364746\n",
      "0 2220 loss - 7.321350574493408\n",
      "0 2230 loss - 7.331881523132324\n",
      "0 2240 loss - 7.5827107429504395\n",
      "0 2250 loss - 7.236549377441406\n",
      "0 2260 loss - 7.444521427154541\n",
      "0 2270 loss - 7.387983322143555\n",
      "0 2280 loss - 7.485693454742432\n",
      "0 2290 loss - 7.429070472717285\n",
      "0 2300 loss - 7.134644985198975\n",
      "0 2310 loss - 7.180501461029053\n",
      "0 2320 loss - 7.3692731857299805\n",
      "0 2330 loss - 7.339606761932373\n",
      "0 2340 loss - 7.3312296867370605\n",
      "0 2350 loss - 7.266601085662842\n",
      "0 2360 loss - 7.570163249969482\n",
      "0 2370 loss - 7.372249603271484\n",
      "0 2380 loss - 7.634577751159668\n",
      "0 2390 loss - 7.742786407470703\n",
      "0 2400 loss - 7.185133457183838\n",
      "0 2410 loss - 7.674415588378906\n",
      "0 2420 loss - 7.629772186279297\n",
      "0 2430 loss - 7.572286605834961\n",
      "0 2440 loss - 7.6285905838012695\n",
      "0 2450 loss - 7.321830749511719\n",
      "0 2460 loss - 7.343952178955078\n",
      "0 2470 loss - 7.571682929992676\n",
      "0 2480 loss - 7.480659008026123\n",
      "0 2490 loss - 7.163180828094482\n",
      "0 2500 loss - 7.448061466217041\n",
      "0 2510 loss - 7.3272929191589355\n",
      "0 2520 loss - 7.491479396820068\n",
      "0 2530 loss - 7.0843281745910645\n",
      "0 2540 loss - 7.272579669952393\n",
      "0 2550 loss - 7.553952217102051\n",
      "0 2560 loss - 7.247035026550293\n",
      "0 2570 loss - 7.429656982421875\n",
      "0 2580 loss - 7.628770351409912\n",
      "0 2590 loss - 7.5585479736328125\n",
      "0 2600 loss - 7.591423511505127\n",
      "0 2610 loss - 7.403336048126221\n",
      "0 2620 loss - 7.477295875549316\n",
      "0 2630 loss - 7.186171531677246\n",
      "0 2640 loss - 7.396785736083984\n",
      "0 2650 loss - 7.5094218254089355\n",
      "0 2660 loss - 7.445297718048096\n",
      "0 2670 loss - 7.501627445220947\n",
      "0 2680 loss - 7.075989723205566\n",
      "0 2690 loss - 7.466795444488525\n",
      "0 2700 loss - 7.2599897384643555\n",
      "0 2710 loss - 6.965463161468506\n",
      "0 2720 loss - 7.596742153167725\n",
      "0 2730 loss - 7.4316182136535645\n",
      "0 2740 loss - 7.535227298736572\n",
      "0 2750 loss - 7.461097717285156\n",
      "0 2760 loss - 7.5334343910217285\n",
      "0 2770 loss - 7.424622535705566\n",
      "0 2780 loss - 6.900697708129883\n",
      "0 2790 loss - 7.445034027099609\n",
      "0 2800 loss - 7.5032758712768555\n",
      "0 2810 loss - 7.310630798339844\n",
      "0 2820 loss - 7.241564750671387\n",
      "0 2830 loss - 7.567011833190918\n",
      "0 2840 loss - 7.557468414306641\n",
      "0 2850 loss - 7.313094139099121\n",
      "0 2860 loss - 7.219546794891357\n",
      "0 2870 loss - 7.162761211395264\n",
      "0 2880 loss - 7.070615768432617\n",
      "0 2890 loss - 7.537985324859619\n",
      "0 2900 loss - 7.152528762817383\n",
      "0 2910 loss - 7.4566264152526855\n",
      "0 2920 loss - 7.389163494110107\n",
      "0 2930 loss - 7.5225749015808105\n",
      "0 2940 loss - 7.54414701461792\n",
      "1 0 loss - 7.164702892303467\n",
      "1 10 loss - 7.553677082061768\n",
      "1 20 loss - 7.5921502113342285\n",
      "1 30 loss - 7.55966854095459\n",
      "1 40 loss - 7.5562944412231445\n",
      "1 50 loss - 7.346879005432129\n",
      "1 60 loss - 7.525085926055908\n",
      "1 70 loss - 6.854859828948975\n",
      "1 80 loss - 6.875097274780273\n",
      "1 90 loss - 7.029179096221924\n",
      "1 100 loss - 7.443973064422607\n",
      "1 110 loss - 7.327850341796875\n",
      "1 120 loss - 7.021958827972412\n",
      "1 130 loss - 7.012004375457764\n",
      "1 140 loss - 7.371801853179932\n",
      "1 150 loss - 7.121546745300293\n",
      "1 160 loss - 7.280447006225586\n",
      "1 170 loss - 7.008664608001709\n",
      "1 180 loss - 7.040604114532471\n",
      "1 190 loss - 7.23352575302124\n",
      "1 200 loss - 7.38511848449707\n",
      "1 210 loss - 7.2253642082214355\n",
      "1 220 loss - 7.576085090637207\n",
      "1 230 loss - 7.468682289123535\n",
      "1 240 loss - 7.252652645111084\n",
      "1 250 loss - 7.259194374084473\n",
      "1 260 loss - 6.938645362854004\n",
      "1 270 loss - 7.1461405754089355\n",
      "1 280 loss - 7.833690643310547\n",
      "1 290 loss - 7.144435405731201\n",
      "1 300 loss - 7.178808689117432\n",
      "1 310 loss - 6.878940582275391\n",
      "1 320 loss - 7.101760387420654\n",
      "1 330 loss - 7.530459880828857\n",
      "1 340 loss - 7.329627990722656\n",
      "1 350 loss - 7.516648292541504\n",
      "1 360 loss - 7.571660041809082\n",
      "1 370 loss - 6.887826919555664\n",
      "1 380 loss - 7.64124059677124\n",
      "1 390 loss - 7.60824728012085\n",
      "1 400 loss - 7.345130443572998\n",
      "1 410 loss - 7.25417947769165\n",
      "1 420 loss - 6.853851318359375\n",
      "1 430 loss - 7.14763879776001\n",
      "1 440 loss - 7.114137172698975\n",
      "1 450 loss - 7.251433372497559\n",
      "1 460 loss - 7.213755130767822\n",
      "1 470 loss - 6.931615352630615\n",
      "1 480 loss - 6.798722743988037\n",
      "1 490 loss - 7.0872392654418945\n",
      "1 500 loss - 6.976980686187744\n",
      "1 510 loss - 7.192225933074951\n",
      "1 520 loss - 7.693273067474365\n",
      "1 530 loss - 6.556969165802002\n",
      "1 540 loss - 7.17903995513916\n",
      "1 550 loss - 7.3026533126831055\n",
      "1 560 loss - 7.22707986831665\n",
      "1 570 loss - 7.574941635131836\n",
      "1 580 loss - 6.95274543762207\n",
      "1 590 loss - 6.958569049835205\n",
      "1 600 loss - 6.79071044921875\n",
      "1 610 loss - 6.895869255065918\n",
      "1 620 loss - 6.750155448913574\n",
      "1 630 loss - 6.559171676635742\n",
      "1 640 loss - 7.726278781890869\n",
      "1 650 loss - 6.809567451477051\n",
      "1 660 loss - 7.088751316070557\n",
      "1 670 loss - 7.216047763824463\n",
      "1 680 loss - 7.3707756996154785\n",
      "1 690 loss - 7.16081428527832\n",
      "1 700 loss - 7.3414812088012695\n",
      "1 710 loss - 6.0826416015625\n",
      "1 720 loss - 7.219494819641113\n",
      "1 730 loss - 6.9528632164001465\n",
      "1 740 loss - 6.641886234283447\n",
      "1 750 loss - 7.410332679748535\n",
      "1 760 loss - 7.255000591278076\n",
      "1 770 loss - 6.203780651092529\n",
      "1 780 loss - 7.334893703460693\n",
      "1 790 loss - 6.946686744689941\n",
      "1 800 loss - 7.21884822845459\n",
      "1 810 loss - 6.939302921295166\n",
      "1 820 loss - 7.245011806488037\n",
      "1 830 loss - 7.169734001159668\n",
      "1 840 loss - 6.821092128753662\n",
      "1 850 loss - 7.238277435302734\n",
      "1 860 loss - 7.431023597717285\n",
      "1 870 loss - 7.039793491363525\n",
      "1 880 loss - 7.444126129150391\n",
      "1 890 loss - 6.70250129699707\n",
      "1 900 loss - 7.421355724334717\n",
      "1 910 loss - 6.428825378417969\n",
      "1 920 loss - 6.5758795738220215\n",
      "1 930 loss - 6.913169860839844\n",
      "1 940 loss - 7.473395824432373\n",
      "1 950 loss - 7.393033027648926\n",
      "1 960 loss - 7.087545871734619\n",
      "1 970 loss - 7.260273456573486\n",
      "1 980 loss - 7.1627092361450195\n",
      "1 990 loss - 7.409450054168701\n",
      "1 1000 loss - 6.958093643188477\n",
      "1 1010 loss - 7.080336570739746\n",
      "1 1020 loss - 6.7053093910217285\n",
      "1 1030 loss - 6.971907615661621\n",
      "1 1040 loss - 7.371301651000977\n",
      "1 1050 loss - 6.776456356048584\n",
      "1 1060 loss - 6.996673583984375\n",
      "1 1070 loss - 6.662892818450928\n",
      "1 1080 loss - 6.93452262878418\n",
      "1 1090 loss - 6.935006141662598\n",
      "1 1100 loss - 6.398446083068848\n",
      "1 1110 loss - 6.881383895874023\n",
      "1 1120 loss - 7.042489528656006\n",
      "1 1130 loss - 6.816927433013916\n",
      "1 1140 loss - 7.097136974334717\n",
      "1 1150 loss - 7.336550712585449\n",
      "1 1160 loss - 7.260570049285889\n",
      "1 1170 loss - 6.6694560050964355\n",
      "1 1180 loss - 6.559924125671387\n",
      "1 1190 loss - 7.240907192230225\n",
      "1 1200 loss - 7.192129135131836\n",
      "1 1210 loss - 6.5505571365356445\n",
      "1 1220 loss - 7.337052822113037\n",
      "1 1230 loss - 7.452447891235352\n",
      "1 1240 loss - 7.316035747528076\n",
      "1 1250 loss - 7.474917888641357\n",
      "1 1260 loss - 7.160226821899414\n",
      "1 1270 loss - 6.964027404785156\n",
      "1 1280 loss - 6.818652629852295\n",
      "1 1290 loss - 7.0421552658081055\n",
      "1 1300 loss - 6.557936668395996\n",
      "1 1310 loss - 6.743828296661377\n",
      "1 1320 loss - 6.659185886383057\n",
      "1 1330 loss - 6.671311855316162\n",
      "1 1340 loss - 6.746111869812012\n",
      "1 1350 loss - 6.875183582305908\n",
      "1 1360 loss - 6.386693954467773\n",
      "1 1370 loss - 6.946494102478027\n",
      "1 1380 loss - 6.976633548736572\n",
      "1 1390 loss - 6.662259578704834\n",
      "1 1400 loss - 6.21372127532959\n",
      "1 1410 loss - 6.9830193519592285\n",
      "1 1420 loss - 6.824135780334473\n",
      "1 1430 loss - 7.010010719299316\n",
      "1 1440 loss - 7.43967342376709\n",
      "1 1450 loss - 6.635725021362305\n",
      "1 1460 loss - 6.63633918762207\n",
      "1 1470 loss - 7.124162673950195\n",
      "1 1480 loss - 6.735501766204834\n",
      "1 1490 loss - 6.742741107940674\n",
      "1 1500 loss - 6.98728084564209\n",
      "1 1510 loss - 7.316752910614014\n",
      "1 1520 loss - 6.836410045623779\n",
      "1 1530 loss - 6.978145122528076\n",
      "1 1540 loss - 6.821774005889893\n",
      "1 1550 loss - 7.09312105178833\n",
      "1 1560 loss - 7.0195536613464355\n",
      "1 1570 loss - 7.457139492034912\n",
      "1 1580 loss - 6.9667582511901855\n",
      "1 1590 loss - 7.040940761566162\n",
      "1 1600 loss - 6.9215617179870605\n",
      "1 1610 loss - 6.394543170928955\n",
      "1 1620 loss - 6.9451704025268555\n",
      "1 1630 loss - 6.689671993255615\n",
      "1 1640 loss - 6.917803764343262\n",
      "1 1650 loss - 6.665470600128174\n",
      "1 1660 loss - 6.261270999908447\n",
      "1 1670 loss - 6.984421253204346\n",
      "1 1680 loss - 6.665543079376221\n",
      "1 1690 loss - 7.209349632263184\n",
      "1 1700 loss - 7.052889823913574\n",
      "1 1710 loss - 7.097301959991455\n",
      "1 1720 loss - 7.141544818878174\n",
      "1 1730 loss - 7.113119602203369\n",
      "1 1740 loss - 7.255558967590332\n",
      "1 1750 loss - 6.884838104248047\n",
      "1 1760 loss - 7.227146625518799\n",
      "1 1770 loss - 6.81533670425415\n",
      "1 1780 loss - 7.3480544090271\n",
      "1 1790 loss - 7.057772636413574\n",
      "1 1800 loss - 6.868513107299805\n",
      "1 1810 loss - 6.887825965881348\n",
      "1 1820 loss - 7.480049133300781\n",
      "1 1830 loss - 5.9909210205078125\n",
      "1 1840 loss - 7.104690074920654\n",
      "1 1850 loss - 6.8961615562438965\n",
      "1 1860 loss - 7.186621189117432\n",
      "1 1870 loss - 6.92454719543457\n",
      "1 1880 loss - 7.404707431793213\n",
      "1 1890 loss - 6.539509296417236\n",
      "1 1900 loss - 6.74479866027832\n",
      "1 1910 loss - 6.695193767547607\n",
      "1 1920 loss - 6.579008102416992\n",
      "1 1930 loss - 6.91193962097168\n",
      "1 1940 loss - 7.002228736877441\n",
      "1 1950 loss - 6.966378688812256\n",
      "1 1960 loss - 6.690026760101318\n",
      "1 1970 loss - 6.682684898376465\n",
      "1 1980 loss - 7.365802764892578\n",
      "1 1990 loss - 6.633019924163818\n",
      "1 2000 loss - 6.593679428100586\n",
      "1 2010 loss - 6.7588348388671875\n",
      "1 2020 loss - 7.074329853057861\n",
      "1 2030 loss - 6.935492038726807\n",
      "1 2040 loss - 6.6580071449279785\n",
      "1 2050 loss - 7.217530727386475\n",
      "1 2060 loss - 7.072659969329834\n",
      "1 2070 loss - 7.029129505157471\n",
      "1 2080 loss - 6.763492584228516\n",
      "1 2090 loss - 7.101841926574707\n",
      "1 2100 loss - 7.092814922332764\n",
      "1 2110 loss - 7.211095809936523\n",
      "1 2120 loss - 7.030745506286621\n",
      "1 2130 loss - 7.4330339431762695\n",
      "1 2140 loss - 7.1706085205078125\n",
      "1 2150 loss - 6.740835189819336\n",
      "1 2160 loss - 6.484707355499268\n",
      "1 2170 loss - 7.069787502288818\n",
      "1 2180 loss - 6.657145977020264\n",
      "1 2190 loss - 6.087121486663818\n",
      "1 2200 loss - 7.344413757324219\n",
      "1 2210 loss - 6.727688789367676\n",
      "1 2220 loss - 6.699880123138428\n",
      "1 2230 loss - 6.937925815582275\n",
      "1 2240 loss - 6.985256671905518\n",
      "1 2250 loss - 6.488949298858643\n",
      "1 2260 loss - 7.024203777313232\n",
      "1 2270 loss - 6.910271167755127\n",
      "1 2280 loss - 6.447009563446045\n",
      "1 2290 loss - 7.127391815185547\n",
      "1 2300 loss - 6.853064060211182\n",
      "1 2310 loss - 7.126987934112549\n",
      "1 2320 loss - 7.332792282104492\n",
      "1 2330 loss - 6.2132134437561035\n",
      "1 2340 loss - 7.366245746612549\n",
      "1 2350 loss - 6.6281023025512695\n",
      "1 2360 loss - 6.601202487945557\n",
      "1 2370 loss - 7.317356586456299\n",
      "1 2380 loss - 6.601799011230469\n",
      "1 2390 loss - 7.183050632476807\n",
      "1 2400 loss - 6.7554826736450195\n",
      "1 2410 loss - 7.1693572998046875\n",
      "1 2420 loss - 7.000054359436035\n",
      "1 2430 loss - 7.216452121734619\n",
      "1 2440 loss - 6.943946361541748\n",
      "1 2450 loss - 6.726879596710205\n",
      "1 2460 loss - 7.392706394195557\n",
      "1 2470 loss - 6.656589031219482\n",
      "1 2480 loss - 6.718980312347412\n",
      "1 2490 loss - 7.270665168762207\n",
      "1 2500 loss - 6.644697666168213\n",
      "1 2510 loss - 6.838249683380127\n",
      "1 2520 loss - 6.47318172454834\n",
      "1 2530 loss - 6.936418533325195\n",
      "1 2540 loss - 6.854719638824463\n",
      "1 2550 loss - 7.136748790740967\n",
      "1 2560 loss - 6.757902145385742\n",
      "1 2570 loss - 7.311734676361084\n",
      "1 2580 loss - 6.469006061553955\n",
      "1 2590 loss - 6.994368553161621\n",
      "1 2600 loss - 7.007304668426514\n",
      "1 2610 loss - 7.471475124359131\n",
      "1 2620 loss - 6.975863933563232\n",
      "1 2630 loss - 7.225043296813965\n",
      "1 2640 loss - 7.213746547698975\n",
      "1 2650 loss - 6.7180609703063965\n",
      "1 2660 loss - 6.994993209838867\n",
      "1 2670 loss - 6.759438991546631\n",
      "1 2680 loss - 6.5543718338012695\n",
      "1 2690 loss - 7.008131504058838\n",
      "1 2700 loss - 6.639645099639893\n",
      "1 2710 loss - 6.863307476043701\n",
      "1 2720 loss - 6.887850761413574\n",
      "1 2730 loss - 6.98291015625\n",
      "1 2740 loss - 6.95587158203125\n",
      "1 2750 loss - 6.989523410797119\n",
      "1 2760 loss - 6.470841884613037\n",
      "1 2770 loss - 6.807709693908691\n",
      "1 2780 loss - 7.02870512008667\n",
      "1 2790 loss - 6.595836639404297\n",
      "1 2800 loss - 7.124935150146484\n",
      "1 2810 loss - 7.3783674240112305\n",
      "1 2820 loss - 6.5673418045043945\n",
      "1 2830 loss - 7.231083869934082\n",
      "1 2840 loss - 6.845925807952881\n",
      "1 2850 loss - 6.739928245544434\n",
      "1 2860 loss - 6.681092739105225\n",
      "1 2870 loss - 7.3628058433532715\n",
      "1 2880 loss - 6.90842342376709\n",
      "1 2890 loss - 7.153458595275879\n",
      "1 2900 loss - 6.8664045333862305\n",
      "1 2910 loss - 6.985406398773193\n",
      "1 2920 loss - 7.277410984039307\n",
      "1 2930 loss - 6.989582061767578\n",
      "1 2940 loss - 6.952377796173096\n",
      "2 0 loss - 6.904881954193115\n",
      "2 10 loss - 6.933868408203125\n",
      "2 20 loss - 7.008828639984131\n",
      "2 30 loss - 6.922392845153809\n",
      "2 40 loss - 7.3166046142578125\n",
      "2 50 loss - 7.034993648529053\n",
      "2 60 loss - 6.967737674713135\n",
      "2 70 loss - 6.564676761627197\n",
      "2 80 loss - 6.892244338989258\n",
      "2 90 loss - 6.73541784286499\n",
      "2 100 loss - 6.740425109863281\n",
      "2 110 loss - 6.566101551055908\n",
      "2 120 loss - 6.732232570648193\n",
      "2 130 loss - 6.018008708953857\n",
      "2 140 loss - 6.198410511016846\n",
      "2 150 loss - 6.52900505065918\n",
      "2 160 loss - 6.7437052726745605\n",
      "2 170 loss - 6.750522136688232\n",
      "2 180 loss - 6.9446611404418945\n",
      "2 190 loss - 7.388373851776123\n",
      "2 200 loss - 6.761614799499512\n",
      "2 210 loss - 6.262454986572266\n",
      "2 220 loss - 6.18973445892334\n",
      "2 230 loss - 6.910682678222656\n",
      "2 240 loss - 7.268320083618164\n",
      "2 250 loss - 6.899933815002441\n",
      "2 260 loss - 6.780696392059326\n",
      "2 270 loss - 6.8729448318481445\n",
      "2 280 loss - 7.227481842041016\n",
      "2 290 loss - 6.872307300567627\n",
      "2 300 loss - 6.134422779083252\n",
      "2 310 loss - 6.426846504211426\n",
      "2 320 loss - 6.564303874969482\n",
      "2 330 loss - 6.850732326507568\n",
      "2 340 loss - 6.418908596038818\n",
      "2 350 loss - 6.858393669128418\n",
      "2 360 loss - 6.7817487716674805\n",
      "2 370 loss - 6.3025803565979\n",
      "2 380 loss - 6.889166831970215\n",
      "2 390 loss - 6.787847995758057\n",
      "2 400 loss - 6.9154276847839355\n",
      "2 410 loss - 6.766470909118652\n",
      "2 420 loss - 6.638216972351074\n",
      "2 430 loss - 6.665831565856934\n",
      "2 440 loss - 6.447582721710205\n",
      "2 450 loss - 6.125701904296875\n",
      "2 460 loss - 7.007436752319336\n",
      "2 470 loss - 7.070131778717041\n",
      "2 480 loss - 6.897999286651611\n",
      "2 490 loss - 6.6620659828186035\n",
      "2 500 loss - 7.051146507263184\n",
      "2 510 loss - 7.269735336303711\n",
      "2 520 loss - 6.4017720222473145\n",
      "2 530 loss - 6.254081726074219\n",
      "2 540 loss - 6.576792240142822\n",
      "2 550 loss - 6.574819087982178\n",
      "2 560 loss - 6.71127462387085\n",
      "2 570 loss - 6.7039713859558105\n",
      "2 580 loss - 6.885480880737305\n",
      "2 590 loss - 6.928966999053955\n",
      "2 600 loss - 7.138987064361572\n",
      "2 610 loss - 7.097327709197998\n",
      "2 620 loss - 7.147386074066162\n",
      "2 630 loss - 6.980347156524658\n",
      "2 640 loss - 6.7214555740356445\n",
      "2 650 loss - 6.795760154724121\n",
      "2 660 loss - 6.875920295715332\n",
      "2 670 loss - 6.511778831481934\n",
      "2 680 loss - 6.645088195800781\n",
      "2 690 loss - 6.541410446166992\n",
      "2 700 loss - 7.311843395233154\n",
      "2 710 loss - 6.185914516448975\n",
      "2 720 loss - 6.687422275543213\n",
      "2 730 loss - 6.296201705932617\n",
      "2 740 loss - 7.014159202575684\n",
      "2 750 loss - 6.711000919342041\n",
      "2 760 loss - 6.858588218688965\n",
      "2 770 loss - 6.847812175750732\n",
      "2 780 loss - 6.992921829223633\n",
      "2 790 loss - 6.695627689361572\n",
      "2 800 loss - 6.9557647705078125\n",
      "2 810 loss - 7.244319915771484\n",
      "2 820 loss - 6.45241117477417\n",
      "2 830 loss - 6.3631181716918945\n",
      "2 840 loss - 6.651041030883789\n",
      "2 850 loss - 6.877119541168213\n",
      "2 860 loss - 7.293796062469482\n",
      "2 870 loss - 7.055084228515625\n",
      "2 880 loss - 7.258011341094971\n",
      "2 890 loss - 7.346388339996338\n",
      "2 900 loss - 6.371955394744873\n",
      "2 910 loss - 6.729119300842285\n",
      "2 920 loss - 6.51491641998291\n",
      "2 930 loss - 6.879230499267578\n",
      "2 940 loss - 6.676915645599365\n",
      "2 950 loss - 6.317289352416992\n",
      "2 960 loss - 6.467131614685059\n",
      "2 970 loss - 7.135021686553955\n",
      "2 980 loss - 6.947289943695068\n",
      "2 990 loss - 6.960082054138184\n",
      "2 1000 loss - 6.642359733581543\n",
      "2 1010 loss - 7.194772243499756\n",
      "2 1020 loss - 7.344122409820557\n",
      "2 1030 loss - 6.077093124389648\n",
      "2 1040 loss - 6.448286056518555\n",
      "2 1050 loss - 6.5933356285095215\n",
      "2 1060 loss - 6.903285026550293\n",
      "2 1070 loss - 7.4125590324401855\n",
      "2 1080 loss - 7.208042144775391\n",
      "2 1090 loss - 6.410248279571533\n",
      "2 1100 loss - 6.709272384643555\n",
      "2 1110 loss - 6.800135612487793\n",
      "2 1120 loss - 7.37892484664917\n",
      "2 1130 loss - 6.202987194061279\n",
      "2 1140 loss - 7.105226516723633\n",
      "2 1150 loss - 5.82927131652832\n",
      "2 1160 loss - 6.3141255378723145\n",
      "2 1170 loss - 6.977422714233398\n",
      "2 1180 loss - 6.290060043334961\n",
      "2 1190 loss - 7.381361961364746\n",
      "2 1200 loss - 7.2787981033325195\n",
      "2 1210 loss - 7.31931209564209\n",
      "2 1220 loss - 7.086776256561279\n",
      "2 1230 loss - 6.437361240386963\n",
      "2 1240 loss - 6.952230930328369\n",
      "2 1250 loss - 6.990213394165039\n",
      "2 1260 loss - 6.986744403839111\n",
      "2 1270 loss - 6.961769104003906\n",
      "2 1280 loss - 7.027340888977051\n",
      "2 1290 loss - 6.3911285400390625\n",
      "2 1300 loss - 6.1730055809021\n",
      "2 1310 loss - 6.8097028732299805\n",
      "2 1320 loss - 6.635256767272949\n",
      "2 1330 loss - 6.84305477142334\n",
      "2 1340 loss - 6.216639518737793\n",
      "2 1350 loss - 6.568039417266846\n",
      "2 1360 loss - 6.183858871459961\n",
      "2 1370 loss - 7.164132595062256\n",
      "2 1380 loss - 6.9184064865112305\n",
      "2 1390 loss - 6.682873249053955\n",
      "2 1400 loss - 6.900465965270996\n",
      "2 1410 loss - 6.8993048667907715\n",
      "2 1420 loss - 7.05895471572876\n",
      "2 1430 loss - 6.165585517883301\n",
      "2 1440 loss - 6.121079921722412\n",
      "2 1450 loss - 6.191781520843506\n",
      "2 1460 loss - 6.805798530578613\n",
      "2 1470 loss - 6.403332710266113\n",
      "2 1480 loss - 6.453336238861084\n",
      "2 1490 loss - 7.108954429626465\n",
      "2 1500 loss - 6.497976303100586\n",
      "2 1510 loss - 6.43487024307251\n",
      "2 1520 loss - 7.015422344207764\n",
      "2 1530 loss - 6.8655242919921875\n",
      "2 1540 loss - 6.71016788482666\n",
      "2 1550 loss - 6.689173221588135\n",
      "2 1560 loss - 7.10221529006958\n",
      "2 1570 loss - 5.770272254943848\n",
      "2 1580 loss - 6.401758670806885\n",
      "2 1590 loss - 6.7058281898498535\n",
      "2 1600 loss - 6.757095813751221\n",
      "2 1610 loss - 5.67823600769043\n",
      "2 1620 loss - 6.656301975250244\n",
      "2 1630 loss - 6.5485520362854\n",
      "2 1640 loss - 6.515059947967529\n",
      "2 1650 loss - 6.695016860961914\n",
      "2 1660 loss - 6.5745320320129395\n",
      "2 1670 loss - 7.380902290344238\n",
      "2 1680 loss - 6.509966850280762\n",
      "2 1690 loss - 6.845378875732422\n",
      "2 1700 loss - 7.212406158447266\n",
      "2 1710 loss - 6.694741725921631\n",
      "2 1720 loss - 7.426177978515625\n",
      "2 1730 loss - 6.375219345092773\n",
      "2 1740 loss - 7.084155082702637\n",
      "2 1750 loss - 6.072425365447998\n",
      "2 1760 loss - 6.901808738708496\n",
      "2 1770 loss - 6.904440879821777\n",
      "2 1780 loss - 6.84252405166626\n",
      "2 1790 loss - 7.264725208282471\n",
      "2 1800 loss - 6.916889190673828\n",
      "2 1810 loss - 7.328888893127441\n",
      "2 1820 loss - 6.526264667510986\n",
      "2 1830 loss - 6.6133503913879395\n",
      "2 1840 loss - 6.992598533630371\n",
      "2 1850 loss - 6.660913944244385\n",
      "2 1860 loss - 6.83251953125\n",
      "2 1870 loss - 6.809401988983154\n",
      "2 1880 loss - 6.732058048248291\n",
      "2 1890 loss - 7.339439868927002\n",
      "2 1900 loss - 6.965947151184082\n",
      "2 1910 loss - 6.131379127502441\n",
      "2 1920 loss - 6.8666300773620605\n",
      "2 1930 loss - 6.602945804595947\n",
      "2 1940 loss - 6.555181503295898\n",
      "2 1950 loss - 6.568524360656738\n",
      "2 1960 loss - 6.351922512054443\n",
      "2 1970 loss - 6.065258026123047\n",
      "2 1980 loss - 6.773965358734131\n",
      "2 1990 loss - 7.079399585723877\n",
      "2 2000 loss - 6.87080192565918\n",
      "2 2010 loss - 6.769991874694824\n",
      "2 2020 loss - 6.72451639175415\n",
      "2 2030 loss - 6.797109127044678\n",
      "2 2040 loss - 6.688175201416016\n",
      "2 2050 loss - 6.485481262207031\n",
      "2 2060 loss - 6.514275550842285\n",
      "2 2070 loss - 6.4134321212768555\n",
      "2 2080 loss - 7.300069808959961\n",
      "2 2090 loss - 6.882617950439453\n",
      "2 2100 loss - 6.975841045379639\n",
      "2 2110 loss - 6.844077110290527\n",
      "2 2120 loss - 6.009231090545654\n",
      "2 2130 loss - 7.324030876159668\n",
      "2 2140 loss - 6.665776252746582\n",
      "2 2150 loss - 6.873054027557373\n",
      "2 2160 loss - 6.931205749511719\n",
      "2 2170 loss - 7.0793538093566895\n",
      "2 2180 loss - 6.776981830596924\n",
      "2 2190 loss - 6.884856700897217\n",
      "2 2200 loss - 6.891551494598389\n",
      "2 2210 loss - 6.883750915527344\n",
      "2 2220 loss - 6.953151702880859\n",
      "2 2230 loss - 6.440454483032227\n",
      "2 2240 loss - 7.125051021575928\n",
      "2 2250 loss - 6.6797356605529785\n",
      "2 2260 loss - 6.170501232147217\n",
      "2 2270 loss - 7.489997386932373\n",
      "2 2280 loss - 6.371613025665283\n",
      "2 2290 loss - 7.109980583190918\n",
      "2 2300 loss - 6.801650524139404\n",
      "2 2310 loss - 6.239623546600342\n",
      "2 2320 loss - 6.786109924316406\n",
      "2 2330 loss - 7.05614709854126\n",
      "2 2340 loss - 6.711916923522949\n",
      "2 2350 loss - 7.410829067230225\n",
      "2 2360 loss - 6.65684700012207\n",
      "2 2370 loss - 7.297906398773193\n",
      "2 2380 loss - 7.283225059509277\n",
      "2 2390 loss - 6.760192394256592\n",
      "2 2400 loss - 5.983978271484375\n",
      "2 2410 loss - 6.895878314971924\n",
      "2 2420 loss - 6.930846691131592\n",
      "2 2430 loss - 7.0414228439331055\n",
      "2 2440 loss - 6.909621715545654\n",
      "2 2450 loss - 6.628246307373047\n",
      "2 2460 loss - 5.8067708015441895\n",
      "2 2470 loss - 6.180309295654297\n",
      "2 2480 loss - 6.44130802154541\n",
      "2 2490 loss - 6.886641979217529\n",
      "2 2500 loss - 6.654476165771484\n",
      "2 2510 loss - 6.665849208831787\n",
      "2 2520 loss - 6.591733932495117\n",
      "2 2530 loss - 6.968985557556152\n",
      "2 2540 loss - 6.189583778381348\n",
      "2 2550 loss - 6.397598743438721\n",
      "2 2560 loss - 6.3240065574646\n",
      "2 2570 loss - 6.602726459503174\n",
      "2 2580 loss - 6.874731540679932\n",
      "2 2590 loss - 6.35775089263916\n",
      "2 2600 loss - 7.109241008758545\n",
      "2 2610 loss - 6.488087177276611\n",
      "2 2620 loss - 7.32407283782959\n",
      "2 2630 loss - 6.600366115570068\n",
      "2 2640 loss - 6.903711795806885\n",
      "2 2650 loss - 7.1827826499938965\n",
      "2 2660 loss - 6.663111209869385\n",
      "2 2670 loss - 5.849127769470215\n",
      "2 2680 loss - 6.084451675415039\n",
      "2 2690 loss - 6.787092208862305\n",
      "2 2700 loss - 6.620043754577637\n",
      "2 2710 loss - 6.671525478363037\n",
      "2 2720 loss - 7.098102569580078\n",
      "2 2730 loss - 6.915616035461426\n",
      "2 2740 loss - 6.56895112991333\n",
      "2 2750 loss - 6.194783687591553\n",
      "2 2760 loss - 6.387110233306885\n",
      "2 2770 loss - 6.34222936630249\n",
      "2 2780 loss - 6.941770076751709\n",
      "2 2790 loss - 6.667050361633301\n",
      "2 2800 loss - 6.474223613739014\n",
      "2 2810 loss - 6.627142906188965\n",
      "2 2820 loss - 7.1258673667907715\n",
      "2 2830 loss - 6.838756561279297\n",
      "2 2840 loss - 6.633232116699219\n",
      "2 2850 loss - 7.055305004119873\n",
      "2 2860 loss - 5.805061340332031\n",
      "2 2870 loss - 6.556923866271973\n",
      "2 2880 loss - 6.4759368896484375\n",
      "2 2890 loss - 6.80221700668335\n",
      "2 2900 loss - 6.722677230834961\n",
      "2 2910 loss - 7.295558929443359\n",
      "2 2920 loss - 6.061838150024414\n",
      "2 2930 loss - 6.33912992477417\n",
      "2 2940 loss - 6.789279937744141\n",
      "3 0 loss - 7.081788063049316\n",
      "3 10 loss - 7.324257850646973\n",
      "3 20 loss - 6.551967144012451\n",
      "3 30 loss - 6.936087608337402\n",
      "3 40 loss - 6.3896942138671875\n",
      "3 50 loss - 6.838496208190918\n",
      "3 60 loss - 6.385164737701416\n",
      "3 70 loss - 6.64459753036499\n",
      "3 80 loss - 6.989177703857422\n",
      "3 90 loss - 6.633876800537109\n",
      "3 100 loss - 7.023374080657959\n",
      "3 110 loss - 6.590200424194336\n",
      "3 120 loss - 6.871298313140869\n",
      "3 130 loss - 5.946422100067139\n",
      "3 140 loss - 6.882817268371582\n",
      "3 150 loss - 6.9748077392578125\n",
      "3 160 loss - 5.971753120422363\n",
      "3 170 loss - 6.680427551269531\n",
      "3 180 loss - 5.903019428253174\n",
      "3 190 loss - 6.447058200836182\n",
      "3 200 loss - 6.926385402679443\n",
      "3 210 loss - 6.493251800537109\n",
      "3 220 loss - 7.196589946746826\n",
      "3 230 loss - 6.7245001792907715\n",
      "3 240 loss - 6.789180755615234\n",
      "3 250 loss - 7.149282932281494\n",
      "3 260 loss - 7.078948974609375\n",
      "3 270 loss - 6.958157062530518\n",
      "3 280 loss - 6.417685508728027\n",
      "3 290 loss - 6.817819118499756\n",
      "3 300 loss - 7.006043910980225\n",
      "3 310 loss - 6.595004558563232\n",
      "3 320 loss - 5.972057342529297\n",
      "3 330 loss - 6.583468437194824\n",
      "3 340 loss - 6.977668762207031\n",
      "3 350 loss - 6.639140605926514\n",
      "3 360 loss - 7.36635160446167\n",
      "3 370 loss - 6.371384620666504\n",
      "3 380 loss - 6.7042412757873535\n",
      "3 390 loss - 6.109542369842529\n",
      "3 400 loss - 7.0730743408203125\n",
      "3 410 loss - 6.867257595062256\n",
      "3 420 loss - 6.809549808502197\n",
      "3 430 loss - 7.294662952423096\n",
      "3 440 loss - 6.631636142730713\n",
      "3 450 loss - 6.248997688293457\n",
      "3 460 loss - 6.637047290802002\n",
      "3 470 loss - 6.088485240936279\n",
      "3 480 loss - 7.219584941864014\n",
      "3 490 loss - 7.093537330627441\n",
      "3 500 loss - 6.7683424949646\n",
      "3 510 loss - 5.3466291427612305\n",
      "3 520 loss - 6.716670989990234\n",
      "3 530 loss - 6.392792224884033\n",
      "3 540 loss - 6.690990447998047\n",
      "3 550 loss - 6.185482501983643\n",
      "3 560 loss - 6.460700035095215\n",
      "3 570 loss - 6.494604587554932\n",
      "3 580 loss - 5.424785614013672\n",
      "3 590 loss - 6.745055675506592\n",
      "3 600 loss - 6.317943572998047\n",
      "3 610 loss - 6.603881359100342\n",
      "3 620 loss - 6.523311138153076\n",
      "3 630 loss - 6.032816410064697\n",
      "3 640 loss - 6.223352909088135\n",
      "3 650 loss - 6.751887321472168\n",
      "3 660 loss - 6.800016403198242\n",
      "3 670 loss - 6.3274827003479\n",
      "3 680 loss - 6.761566638946533\n",
      "3 690 loss - 6.347528457641602\n",
      "3 700 loss - 7.132681369781494\n",
      "3 710 loss - 6.524381160736084\n",
      "3 720 loss - 5.975919246673584\n",
      "3 730 loss - 6.768246173858643\n",
      "3 740 loss - 6.6971235275268555\n",
      "3 750 loss - 6.4715375900268555\n",
      "3 760 loss - 6.618710994720459\n",
      "3 770 loss - 7.135857582092285\n",
      "3 780 loss - 6.80016565322876\n",
      "3 790 loss - 6.186279296875\n",
      "3 800 loss - 7.16316556930542\n",
      "3 810 loss - 6.750485420227051\n",
      "3 820 loss - 6.1671462059021\n",
      "3 830 loss - 6.812496185302734\n",
      "3 840 loss - 6.366727828979492\n",
      "3 850 loss - 6.649501800537109\n",
      "3 860 loss - 6.718507289886475\n",
      "3 870 loss - 6.562546730041504\n",
      "3 880 loss - 6.894013404846191\n",
      "3 890 loss - 6.487794399261475\n",
      "3 900 loss - 6.220686435699463\n",
      "3 910 loss - 7.141727447509766\n",
      "3 920 loss - 6.7590718269348145\n",
      "3 930 loss - 7.016378879547119\n",
      "3 940 loss - 6.646728515625\n",
      "3 950 loss - 6.764477729797363\n",
      "3 960 loss - 6.409900188446045\n",
      "3 970 loss - 7.052161693572998\n",
      "3 980 loss - 6.576249122619629\n",
      "3 990 loss - 6.714179039001465\n",
      "3 1000 loss - 7.214410781860352\n",
      "3 1010 loss - 6.33650016784668\n",
      "3 1020 loss - 6.983502388000488\n",
      "3 1030 loss - 7.015566825866699\n",
      "3 1040 loss - 6.354558944702148\n",
      "3 1050 loss - 6.3904523849487305\n"
     ]
    }
   ],
   "source": [
    "from random import choice\n",
    "from pathlib import Path\n",
    "\n",
    "# torch\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "# vision imports\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import transforms as T\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "# dalle related classes and utils\n",
    "\n",
    "from dalle_pytorch import OpenAIDiscreteVAE, DiscreteVAE, DALLE\n",
    "from dalle_pytorch.tokenizer import tokenizer, HugTokenizer\n",
    "\n",
    "# helpers\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "# argument parsing\n",
    "\n",
    "VAE_PATH = None   # './vae.pt' - will use OpenAIs pretrained VAE if not set\n",
    "DALLE_PATH = None # './dalle.pt'\n",
    "TAMING = False    # use VAE from taming transformers paper\n",
    "IMAGE_TEXT_FOLDER = './'\n",
    "BPE_PATH = None\n",
    "RESUME = exists(DALLE_PATH)\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 3e-4\n",
    "GRAD_CLIP_NORM = 0.5\n",
    "\n",
    "MODEL_DIM = 512\n",
    "TEXT_SEQ_LEN = 256\n",
    "DEPTH = 2\n",
    "HEADS = 4\n",
    "DIM_HEAD = 64\n",
    "REVERSIBLE = True\n",
    "\n",
    "# tokenizer\n",
    "\n",
    "if BPE_PATH is not None:\n",
    "    tokenizer = HugTokenizer(BPE_PATH)\n",
    "\n",
    "# reconstitute vae\n",
    "\n",
    "if RESUME:\n",
    "    dalle_path = Path(DALLE_PATH)\n",
    "    assert dalle_path.exists(), 'DALL-E model file does not exist'\n",
    "\n",
    "    loaded_obj = torch.load(str(dalle_path))\n",
    "\n",
    "    dalle_params, vae_params, weights = loaded_obj['hparams'], loaded_obj['vae_params'], loaded_obj['weights']\n",
    "\n",
    "    if vae_params is not None:\n",
    "        vae = DiscreteVAE(**vae_params)\n",
    "    else:\n",
    "        vae = OpenAIDiscreteVAE()\n",
    "\n",
    "    dalle_params = dict(        \n",
    "        **dalle_params\n",
    "    )\n",
    "\n",
    "    IMAGE_SIZE = vae.image_size\n",
    "\n",
    "else:\n",
    "    if exists(VAE_PATH):\n",
    "        vae_path = Path(VAE_PATH)\n",
    "        assert vae_path.exists(), 'VAE model file does not exist'\n",
    "\n",
    "        loaded_obj = torch.load(str(vae_path))\n",
    "\n",
    "        vae_params, weights = loaded_obj['hparams'], loaded_obj['weights']\n",
    "\n",
    "        vae = DiscreteVAE(**vae_params)\n",
    "        vae.load_state_dict(weights)\n",
    "    else:\n",
    "        print('using pretrained VAE for encoding images to tokens')\n",
    "        vae_params = None\n",
    "\n",
    "        vae_klass = OpenAIDiscreteVAE if not TAMING else VQGanVAE1024\n",
    "        vae = vae_klass()\n",
    "\n",
    "    IMAGE_SIZE = vae.image_size\n",
    "\n",
    "    dalle_params = dict(\n",
    "        num_text_tokens = tokenizer.vocab_size,\n",
    "        text_seq_len = TEXT_SEQ_LEN,\n",
    "        dim = MODEL_DIM,\n",
    "        depth = DEPTH,\n",
    "        heads = HEADS,\n",
    "        dim_head = DIM_HEAD,\n",
    "        reversible = REVERSIBLE\n",
    "    )\n",
    "\n",
    "# helpers\n",
    "\n",
    "def save_model(path):\n",
    "    save_obj = {\n",
    "        'hparams': dalle_params,\n",
    "        'vae_params': vae_params,\n",
    "        'weights': dalle.state_dict()\n",
    "    }\n",
    "\n",
    "    torch.save(save_obj, path)\n",
    "\n",
    "# dataset loading\n",
    "\n",
    "class TextImageDataset(Dataset):\n",
    "    def __init__(self, folder, text_len = 256, image_size = 128):\n",
    "        super().__init__()\n",
    "        path = Path(folder)\n",
    "\n",
    "        text_files = [*path.glob('**/*.txt')]\n",
    "\n",
    "        image_files = [\n",
    "            *path.glob('**/*.png'),\n",
    "            *path.glob('**/*.jpg'),\n",
    "            *path.glob('**/*.jpeg')\n",
    "        ]\n",
    "\n",
    "        text_files = {t.stem: t for t in text_files}\n",
    "        image_files = {i.stem: i for i in image_files}\n",
    "\n",
    "        keys = (image_files.keys() & text_files.keys())\n",
    "\n",
    "        self.keys = list(keys)\n",
    "        self.text_files = {k: v for k, v in text_files.items() if k in keys}\n",
    "        self.image_files = {k: v for k, v in image_files.items() if k in keys}\n",
    "\n",
    "        self.image_tranform = T.Compose([\n",
    "            T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "            T.RandomResizedCrop(image_size, scale = (0.75, 1.), ratio = (1., 1.)),\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        key = self.keys[ind]\n",
    "        text_file = self.text_files[key]\n",
    "        image_file = self.image_files[key]\n",
    "\n",
    "        image = Image.open(image_file)\n",
    "        descriptions = text_file.read_text().split('\\n')\n",
    "        descriptions = list(filter(lambda t: len(t) > 0, descriptions))\n",
    "        description = choice(descriptions)\n",
    "\n",
    "        tokenized_text = tokenizer.tokenize(description).squeeze(0)\n",
    "        mask = tokenized_text != 0\n",
    "\n",
    "        image_tensor = self.image_tranform(image)\n",
    "        return tokenized_text, image_tensor, mask\n",
    "\n",
    "# create dataset and dataloader\n",
    "\n",
    "ds = TextImageDataset(\n",
    "    IMAGE_TEXT_FOLDER,\n",
    "    text_len = TEXT_SEQ_LEN,\n",
    "    image_size = IMAGE_SIZE\n",
    ")\n",
    "\n",
    "assert len(ds) > 0, 'dataset is empty'\n",
    "print(f'{len(ds)} image-text pairs found for training')\n",
    "\n",
    "dl = DataLoader(ds, batch_size = BATCH_SIZE, shuffle = True, drop_last = True)\n",
    "\n",
    "# initialize DALL-E\n",
    "\n",
    "dalle = DALLE(vae = vae, **dalle_params).cuda()\n",
    "\n",
    "if RESUME:\n",
    "    dalle.load_state_dict(weights)\n",
    "\n",
    "# optimizer\n",
    "\n",
    "opt = Adam(dalle.parameters(), lr = LEARNING_RATE)\n",
    "\n",
    "# experiment tracker\n",
    "\n",
    "import wandb\n",
    "\n",
    "model_config = dict(\n",
    "    depth = DEPTH,\n",
    "    heads = HEADS,\n",
    "    dim_head = DIM_HEAD\n",
    ")\n",
    "\n",
    "run = wandb.init(project = 'dalle_train_transformer', resume = RESUME, config = model_config)\n",
    "\n",
    "# training\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for i, (text, images, mask) in enumerate(dl):\n",
    "        text, images, mask = map(lambda t: t.cuda(), (text, images, mask))\n",
    "\n",
    "        loss = dalle(text, images, mask = mask, return_loss = True)\n",
    "\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(dalle.parameters(), GRAD_CLIP_NORM)\n",
    "\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "        log = {}\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(epoch, i, f'loss - {loss.item()}')\n",
    "\n",
    "            log = {\n",
    "                **log,\n",
    "                'epoch': epoch,\n",
    "                'iter': i,\n",
    "                'loss': loss.item()\n",
    "            }\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            sample_text = text[:1]\n",
    "            token_list = sample_text.masked_select(sample_text != 0).tolist()\n",
    "            decoded_text = tokenizer.decode(token_list)\n",
    "\n",
    "            image = dalle.generate_images(\n",
    "                text[:1],\n",
    "                mask = mask[:1],\n",
    "                filter_thres = 0.9    # topk sampling at 0.9\n",
    "            )\n",
    "\n",
    "            save_model(f'./dalle.pt')\n",
    "            wandb.save(f'./dalle.pt')\n",
    "\n",
    "            log = {\n",
    "                **log,\n",
    "                'image': wandb.Image(image, caption = decoded_text)\n",
    "            }\n",
    "\n",
    "        wandb.log(log)\n",
    "\n",
    "    # save trained model to wandb as an artifact every epoch's end\n",
    "\n",
    "    model_artifact = wandb.Artifact('trained-dalle', type = 'model', metadata = dict(model_config))\n",
    "    model_artifact.add_file('dalle.pt')\n",
    "    run.log_artifact(model_artifact)\n",
    "\n",
    "save_model(f'./dalle-final.pt')\n",
    "wandb.save('./dalle-final.pt')\n",
    "model_artifact = wandb.Artifact('trained-dalle', type = 'model', metadata = dict(model_config))\n",
    "model_artifact.add_file('dalle-final.pt')\n",
    "run.log_artifact(model_artifact)\n",
    "\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
